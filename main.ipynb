{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data_handlers import tokenise, batch, get_batch\n",
    "import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "path = './data/penn/'\n",
    "batch_size = 40\n",
    "\n",
    "emsize = 400\n",
    "nhid = 1150\n",
    "nlayers = 3\n",
    "dropout = 0.5\n",
    "tied = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "dictionary = data_handlers.Dictionary()\n",
    "\n",
    "# Tokenise data to replace characters with integer indexes\n",
    "train_data, dictionary = tokenise(path+'train.txt', dictionary)\n",
    "val_data, dictionary   = tokenise(path+'valid.txt', dictionary)\n",
    "test_data, dictionary  = tokenise(path+'test.txt', dictionary)\n",
    "\n",
    "# Batch data: reshapes vector as matrix where number of columns j \n",
    "# is the batch size.\n",
    "train_data = batch(train_data, batch_size)\n",
    "val_data = batch(val_data, batch_size)\n",
    "test_data  = batch(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD MODEL\n",
    "\n",
    "ntokens = len(dictionary)\n",
    "LSTM = rnn.LSTMModel(ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
    "\n",
    "# TODO: Check loss matches paper\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING CODE\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "\n",
    "def evaluate(model, data, ntokens, batch_size, bptt):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data.size(0) - 1, bptt):\n",
    "            x, y = get_batch(data, i)\n",
    "            output, hidden = model(x, y)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(x) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / (len(data) - 1)\n",
    "            \n",
    "    \n",
    "def train(model, data, ntokens:int, batch_size:int, lr:float, bptt:int, clip):\n",
    "    log_interval = 1\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, data.size(0)-1, bptt)):\n",
    "        inputs, targets = get_batch(data, i, bptt)\n",
    "        #Â For each batch, detach hidden state from state created in previous\n",
    "        # batches. Else, the model would attempt backpropagation through the \n",
    "        # entire dataset\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        # Zero the gradients from previous iteration, ready for new values\n",
    "        model.zero_grad()\n",
    "        # Forward pass\n",
    "        output, hidden = model(inputs, hidden)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # TODO: Check clipping config\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed  = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(data) // bptt, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/  663 batches | lr 0.40 | ms/batch 12084.74 | loss 18.39 | ppl 97211984.22\n",
      "| epoch   1 |     2/  663 batches | lr 0.40 | ms/batch 5498.58 | loss  9.17 | ppl  9590.28\n",
      "| epoch   1 |     3/  663 batches | lr 0.40 | ms/batch 5407.46 | loss  9.15 | ppl  9416.59\n",
      "| epoch   1 |     4/  663 batches | lr 0.40 | ms/batch 5367.70 | loss  9.13 | ppl  9262.14\n",
      "| epoch   1 |     5/  663 batches | lr 0.40 | ms/batch 5341.70 | loss  9.11 | ppl  9025.98\n",
      "| epoch   1 |     6/  663 batches | lr 0.40 | ms/batch 5333.58 | loss  9.09 | ppl  8835.90\n",
      "| epoch   1 |     7/  663 batches | lr 0.40 | ms/batch 5376.67 | loss  9.07 | ppl  8709.60\n",
      "| epoch   1 |     8/  663 batches | lr 0.40 | ms/batch 5452.73 | loss  9.05 | ppl  8533.27\n",
      "| epoch   1 |     9/  663 batches | lr 0.40 | ms/batch 5588.42 | loss  9.02 | ppl  8255.66\n",
      "| epoch   1 |    10/  663 batches | lr 0.40 | ms/batch 5344.47 | loss  8.98 | ppl  7977.74\n",
      "| epoch   1 |    11/  663 batches | lr 0.40 | ms/batch 5385.25 | loss  8.96 | ppl  7802.01\n",
      "| epoch   1 |    12/  663 batches | lr 0.40 | ms/batch 5377.39 | loss  8.92 | ppl  7472.15\n",
      "| epoch   1 |    13/  663 batches | lr 0.40 | ms/batch 5807.52 | loss  8.90 | ppl  7325.85\n",
      "| epoch   1 |    14/  663 batches | lr 0.40 | ms/batch 5365.64 | loss  8.87 | ppl  7110.40\n",
      "| epoch   1 |    15/  663 batches | lr 0.40 | ms/batch 5339.90 | loss  8.86 | ppl  7055.11\n",
      "| epoch   1 |    16/  663 batches | lr 0.40 | ms/batch 5358.67 | loss  8.83 | ppl  6832.90\n",
      "| epoch   1 |    17/  663 batches | lr 0.40 | ms/batch 5471.78 | loss  8.80 | ppl  6601.16\n",
      "| epoch   1 |    18/  663 batches | lr 0.40 | ms/batch 5360.62 | loss  8.72 | ppl  6094.98\n",
      "| epoch   1 |    19/  663 batches | lr 0.40 | ms/batch 5463.14 | loss  8.64 | ppl  5641.61\n",
      "| epoch   1 |    20/  663 batches | lr 0.40 | ms/batch 5356.74 | loss  8.62 | ppl  5543.76\n",
      "| epoch   1 |    21/  663 batches | lr 0.40 | ms/batch 5359.56 | loss  8.56 | ppl  5207.73\n",
      "| epoch   1 |    22/  663 batches | lr 0.40 | ms/batch 5365.00 | loss  8.49 | ppl  4884.06\n",
      "| epoch   1 |    23/  663 batches | lr 0.40 | ms/batch 5794.06 | loss  8.42 | ppl  4556.46\n",
      "| epoch   1 |    24/  663 batches | lr 0.40 | ms/batch 5643.55 | loss  8.24 | ppl  3803.69\n",
      "| epoch   1 |    25/  663 batches | lr 0.40 | ms/batch 5399.63 | loss  8.14 | ppl  3424.10\n",
      "| epoch   1 |    26/  663 batches | lr 0.40 | ms/batch 5381.84 | loss  8.05 | ppl  3126.44\n",
      "| epoch   1 |    27/  663 batches | lr 0.40 | ms/batch 5395.40 | loss  7.95 | ppl  2826.07\n",
      "| epoch   1 |    28/  663 batches | lr 0.40 | ms/batch 5404.99 | loss  7.89 | ppl  2672.27\n",
      "| epoch   1 |    29/  663 batches | lr 0.40 | ms/batch 5372.77 | loss  7.92 | ppl  2764.69\n",
      "| epoch   1 |    30/  663 batches | lr 0.40 | ms/batch 5447.85 | loss  7.80 | ppl  2451.18\n",
      "| epoch   1 |    31/  663 batches | lr 0.40 | ms/batch 5425.22 | loss  7.82 | ppl  2499.26\n",
      "| epoch   1 |    32/  663 batches | lr 0.40 | ms/batch 5397.17 | loss  7.67 | ppl  2153.54\n",
      "| epoch   1 |    33/  663 batches | lr 0.40 | ms/batch 5948.44 | loss  7.67 | ppl  2134.98\n",
      "| epoch   1 |    34/  663 batches | lr 0.40 | ms/batch 5759.66 | loss  7.63 | ppl  2067.68\n",
      "| epoch   1 |    35/  663 batches | lr 0.40 | ms/batch 5575.87 | loss  7.76 | ppl  2339.56\n",
      "| epoch   1 |    36/  663 batches | lr 0.40 | ms/batch 5569.85 | loss  7.73 | ppl  2277.25\n",
      "| epoch   1 |    37/  663 batches | lr 0.40 | ms/batch 5535.93 | loss  7.50 | ppl  1810.21\n",
      "| epoch   1 |    38/  663 batches | lr 0.40 | ms/batch 5584.04 | loss  7.67 | ppl  2136.43\n",
      "| epoch   1 |    39/  663 batches | lr 0.40 | ms/batch 5543.02 | loss  7.58 | ppl  1954.14\n",
      "| epoch   1 |    40/  663 batches | lr 0.40 | ms/batch 5619.46 | loss  7.60 | ppl  1990.23\n",
      "| epoch   1 |    41/  663 batches | lr 0.40 | ms/batch 5536.98 | loss  7.57 | ppl  1931.19\n",
      "| epoch   1 |    42/  663 batches | lr 0.40 | ms/batch 5566.33 | loss  7.48 | ppl  1768.87\n",
      "| epoch   1 |    43/  663 batches | lr 0.40 | ms/batch 5552.78 | loss  7.56 | ppl  1914.74\n",
      "| epoch   1 |    44/  663 batches | lr 0.40 | ms/batch 5658.23 | loss  7.49 | ppl  1791.28\n",
      "| epoch   1 |    45/  663 batches | lr 0.40 | ms/batch 5934.76 | loss  7.43 | ppl  1685.84\n",
      "| epoch   1 |    46/  663 batches | lr 0.40 | ms/batch 5633.42 | loss  7.40 | ppl  1632.90\n",
      "| epoch   1 |    47/  663 batches | lr 0.40 | ms/batch 5571.73 | loss  7.40 | ppl  1641.37\n",
      "| epoch   1 |    48/  663 batches | lr 0.40 | ms/batch 5583.74 | loss  7.45 | ppl  1720.26\n",
      "| epoch   1 |    49/  663 batches | lr 0.40 | ms/batch 5571.21 | loss  7.43 | ppl  1692.70\n",
      "| epoch   1 |    50/  663 batches | lr 0.40 | ms/batch 5596.19 | loss  7.40 | ppl  1633.49\n",
      "| epoch   1 |    51/  663 batches | lr 0.40 | ms/batch 5561.63 | loss  7.24 | ppl  1393.13\n",
      "| epoch   1 |    52/  663 batches | lr 0.40 | ms/batch 5670.81 | loss  7.43 | ppl  1679.41\n",
      "| epoch   1 |    53/  663 batches | lr 0.40 | ms/batch 5685.82 | loss  7.31 | ppl  1493.96\n",
      "| epoch   1 |    54/  663 batches | lr 0.40 | ms/batch 5647.80 | loss  7.42 | ppl  1663.77\n",
      "| epoch   1 |    55/  663 batches | lr 0.40 | ms/batch 5602.77 | loss  7.41 | ppl  1648.23\n",
      "| epoch   1 |    56/  663 batches | lr 0.40 | ms/batch 5593.96 | loss  7.26 | ppl  1423.31\n",
      "| epoch   1 |    57/  663 batches | lr 0.40 | ms/batch 5503.38 | loss  7.32 | ppl  1504.10\n",
      "| epoch   1 |    58/  663 batches | lr 0.40 | ms/batch 5531.61 | loss  7.17 | ppl  1295.39\n",
      "| epoch   1 |    59/  663 batches | lr 0.40 | ms/batch 5502.83 | loss  7.31 | ppl  1494.16\n",
      "| epoch   1 |    60/  663 batches | lr 0.40 | ms/batch 5532.51 | loss  7.20 | ppl  1340.26\n",
      "| epoch   1 |    61/  663 batches | lr 0.40 | ms/batch 5502.17 | loss  7.26 | ppl  1416.03\n",
      "| epoch   1 |    62/  663 batches | lr 0.40 | ms/batch 5595.90 | loss  7.09 | ppl  1204.83\n",
      "| epoch   1 |    63/  663 batches | lr 0.40 | ms/batch 5709.92 | loss  7.29 | ppl  1463.29\n",
      "| epoch   1 |    64/  663 batches | lr 0.40 | ms/batch 5571.31 | loss  7.29 | ppl  1472.13\n",
      "| epoch   1 |    65/  663 batches | lr 0.40 | ms/batch 5517.48 | loss  7.07 | ppl  1173.10\n",
      "| epoch   1 |    66/  663 batches | lr 0.40 | ms/batch 6032.68 | loss  7.01 | ppl  1110.76\n",
      "| epoch   1 |    67/  663 batches | lr 0.40 | ms/batch 5630.38 | loss  7.19 | ppl  1331.90\n",
      "| epoch   1 |    68/  663 batches | lr 0.40 | ms/batch 5721.17 | loss  7.32 | ppl  1513.90\n",
      "| epoch   1 |    69/  663 batches | lr 0.40 | ms/batch 5545.89 | loss  7.03 | ppl  1125.37\n",
      "| epoch   1 |    70/  663 batches | lr 0.40 | ms/batch 5556.83 | loss  7.20 | ppl  1339.68\n",
      "| epoch   1 |    71/  663 batches | lr 0.40 | ms/batch 5529.23 | loss  7.16 | ppl  1288.42\n",
      "| epoch   1 |    72/  663 batches | lr 0.40 | ms/batch 5572.79 | loss  7.27 | ppl  1432.87\n",
      "| epoch   1 |    73/  663 batches | lr 0.40 | ms/batch 5497.59 | loss  7.17 | ppl  1295.46\n",
      "| epoch   1 |    74/  663 batches | lr 0.40 | ms/batch 5566.46 | loss  7.04 | ppl  1145.60\n",
      "| epoch   1 |    75/  663 batches | lr 0.40 | ms/batch 5483.49 | loss  7.26 | ppl  1417.93\n",
      "| epoch   1 |    76/  663 batches | lr 0.40 | ms/batch 5672.70 | loss  7.13 | ppl  1254.05\n",
      "| epoch   1 |    77/  663 batches | lr 0.40 | ms/batch 5622.43 | loss  7.11 | ppl  1223.04\n",
      "| epoch   1 |    78/  663 batches | lr 0.40 | ms/batch 5627.12 | loss  7.17 | ppl  1293.95\n",
      "| epoch   1 |    79/  663 batches | lr 0.40 | ms/batch 5577.15 | loss  7.10 | ppl  1214.90\n",
      "| epoch   1 |    80/  663 batches | lr 0.40 | ms/batch 5599.41 | loss  7.17 | ppl  1298.11\n",
      "| epoch   1 |    81/  663 batches | lr 0.40 | ms/batch 5568.65 | loss  7.05 | ppl  1152.67\n",
      "| epoch   1 |    82/  663 batches | lr 0.40 | ms/batch 5591.78 | loss  6.93 | ppl  1021.40\n",
      "| epoch   1 |    83/  663 batches | lr 0.40 | ms/batch 5602.12 | loss  7.08 | ppl  1186.50\n",
      "| epoch   1 |    84/  663 batches | lr 0.40 | ms/batch 5614.33 | loss  7.24 | ppl  1387.39\n",
      "| epoch   1 |    85/  663 batches | lr 0.40 | ms/batch 5571.98 | loss  7.10 | ppl  1207.61\n",
      "| epoch   1 |    86/  663 batches | lr 0.40 | ms/batch 5617.28 | loss  6.97 | ppl  1060.91\n",
      "| epoch   1 |    87/  663 batches | lr 0.40 | ms/batch 5806.56 | loss  7.09 | ppl  1199.39\n",
      "| epoch   1 |    88/  663 batches | lr 0.40 | ms/batch 5761.79 | loss  7.16 | ppl  1280.80\n",
      "| epoch   1 |    89/  663 batches | lr 0.40 | ms/batch 5566.09 | loss  7.11 | ppl  1221.67\n",
      "| epoch   1 |    90/  663 batches | lr 0.40 | ms/batch 5567.73 | loss  7.14 | ppl  1263.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    91/  663 batches | lr 0.40 | ms/batch 5505.72 | loss  7.13 | ppl  1242.88\n",
      "| epoch   1 |    92/  663 batches | lr 0.40 | ms/batch 5530.52 | loss  7.03 | ppl  1126.67\n",
      "| epoch   1 |    93/  663 batches | lr 0.40 | ms/batch 5489.38 | loss  6.92 | ppl  1009.15\n",
      "| epoch   1 |    94/  663 batches | lr 0.40 | ms/batch 5537.29 | loss  6.90 | ppl   990.15\n",
      "| epoch   1 |    95/  663 batches | lr 0.40 | ms/batch 5549.37 | loss  6.96 | ppl  1053.10\n",
      "| epoch   1 |    96/  663 batches | lr 0.40 | ms/batch 5597.04 | loss  6.87 | ppl   960.76\n",
      "| epoch   1 |    97/  663 batches | lr 0.40 | ms/batch 5570.13 | loss  6.89 | ppl   983.74\n",
      "| epoch   1 |    98/  663 batches | lr 0.40 | ms/batch 5611.32 | loss  7.06 | ppl  1160.05\n",
      "| epoch   1 |    99/  663 batches | lr 0.40 | ms/batch 5605.87 | loss  7.16 | ppl  1286.23\n",
      "| epoch   1 |   100/  663 batches | lr 0.40 | ms/batch 5671.09 | loss  6.99 | ppl  1081.41\n",
      "| epoch   1 |   101/  663 batches | lr 0.40 | ms/batch 5686.24 | loss  7.00 | ppl  1098.69\n",
      "| epoch   1 |   102/  663 batches | lr 0.40 | ms/batch 5623.49 | loss  6.99 | ppl  1082.15\n",
      "| epoch   1 |   103/  663 batches | lr 0.40 | ms/batch 5588.79 | loss  6.96 | ppl  1056.11\n",
      "| epoch   1 |   104/  663 batches | lr 0.40 | ms/batch 5622.55 | loss  7.05 | ppl  1158.11\n",
      "| epoch   1 |   105/  663 batches | lr 0.40 | ms/batch 5645.59 | loss  7.07 | ppl  1179.11\n",
      "| epoch   1 |   106/  663 batches | lr 0.40 | ms/batch 5628.13 | loss  7.09 | ppl  1205.63\n",
      "| epoch   1 |   107/  663 batches | lr 0.40 | ms/batch 5575.09 | loss  6.92 | ppl  1014.31\n",
      "| epoch   1 |   108/  663 batches | lr 0.40 | ms/batch 5662.07 | loss  7.04 | ppl  1140.75\n",
      "| epoch   1 |   109/  663 batches | lr 0.40 | ms/batch 5941.06 | loss  7.03 | ppl  1131.82\n",
      "| epoch   1 |   110/  663 batches | lr 0.40 | ms/batch 6117.13 | loss  7.10 | ppl  1217.28\n",
      "| epoch   1 |   111/  663 batches | lr 0.40 | ms/batch 5740.59 | loss  6.98 | ppl  1073.07\n",
      "| epoch   1 |   112/  663 batches | lr 0.40 | ms/batch 5759.94 | loss  6.89 | ppl   986.73\n",
      "| epoch   1 |   113/  663 batches | lr 0.40 | ms/batch 5673.12 | loss  7.08 | ppl  1184.89\n",
      "| epoch   1 |   114/  663 batches | lr 0.40 | ms/batch 5680.06 | loss  6.97 | ppl  1066.41\n",
      "| epoch   1 |   115/  663 batches | lr 0.40 | ms/batch 5692.83 | loss  6.97 | ppl  1063.37\n",
      "| epoch   1 |   116/  663 batches | lr 0.40 | ms/batch 5665.69 | loss  6.92 | ppl  1010.26\n",
      "| epoch   1 |   117/  663 batches | lr 0.40 | ms/batch 5668.96 | loss  7.04 | ppl  1143.42\n",
      "| epoch   1 |   118/  663 batches | lr 0.40 | ms/batch 5685.82 | loss  6.89 | ppl   986.68\n",
      "| epoch   1 |   119/  663 batches | lr 0.40 | ms/batch 5670.57 | loss  6.99 | ppl  1080.82\n",
      "| epoch   1 |   120/  663 batches | lr 0.40 | ms/batch 5706.24 | loss  7.05 | ppl  1154.69\n",
      "| epoch   1 |   121/  663 batches | lr 0.40 | ms/batch 5660.93 | loss  7.00 | ppl  1098.78\n",
      "| epoch   1 |   122/  663 batches | lr 0.40 | ms/batch 5687.62 | loss  7.07 | ppl  1181.40\n",
      "| epoch   1 |   123/  663 batches | lr 0.40 | ms/batch 5681.32 | loss  7.06 | ppl  1162.93\n",
      "| epoch   1 |   124/  663 batches | lr 0.40 | ms/batch 5649.45 | loss  6.93 | ppl  1019.22\n",
      "| epoch   1 |   125/  663 batches | lr 0.40 | ms/batch 5666.29 | loss  6.94 | ppl  1035.75\n",
      "| epoch   1 |   126/  663 batches | lr 0.40 | ms/batch 5738.46 | loss  6.97 | ppl  1060.54\n",
      "| epoch   1 |   127/  663 batches | lr 0.40 | ms/batch 5853.22 | loss  6.95 | ppl  1047.71\n",
      "| epoch   1 |   128/  663 batches | lr 0.40 | ms/batch 5694.58 | loss  6.98 | ppl  1074.26\n",
      "| epoch   1 |   129/  663 batches | lr 0.40 | ms/batch 5707.24 | loss  7.04 | ppl  1135.72\n",
      "| epoch   1 |   130/  663 batches | lr 0.40 | ms/batch 5755.36 | loss  7.03 | ppl  1133.42\n",
      "| epoch   1 |   131/  663 batches | lr 0.40 | ms/batch 5722.28 | loss  7.08 | ppl  1187.62\n",
      "| epoch   1 |   132/  663 batches | lr 0.40 | ms/batch 5599.60 | loss  6.94 | ppl  1031.69\n",
      "| epoch   1 |   133/  663 batches | lr 0.40 | ms/batch 5604.17 | loss  6.99 | ppl  1089.05\n",
      "| epoch   1 |   134/  663 batches | lr 0.40 | ms/batch 5681.52 | loss  7.04 | ppl  1137.65\n",
      "| epoch   1 |   135/  663 batches | lr 0.40 | ms/batch 5780.02 | loss  6.85 | ppl   946.84\n",
      "| epoch   1 |   136/  663 batches | lr 0.40 | ms/batch 5782.32 | loss  6.99 | ppl  1085.84\n",
      "| epoch   1 |   137/  663 batches | lr 0.40 | ms/batch 5805.15 | loss  6.97 | ppl  1063.63\n",
      "| epoch   1 |   138/  663 batches | lr 0.40 | ms/batch 5715.90 | loss  7.02 | ppl  1113.29\n",
      "| epoch   1 |   139/  663 batches | lr 0.40 | ms/batch 5749.01 | loss  7.07 | ppl  1174.80\n",
      "| epoch   1 |   140/  663 batches | lr 0.40 | ms/batch 5697.11 | loss  7.02 | ppl  1118.53\n",
      "| epoch   1 |   141/  663 batches | lr 0.40 | ms/batch 5689.78 | loss  6.91 | ppl  1004.49\n",
      "| epoch   1 |   142/  663 batches | lr 0.40 | ms/batch 5676.10 | loss  6.98 | ppl  1079.98\n",
      "| epoch   1 |   143/  663 batches | lr 0.40 | ms/batch 5687.82 | loss  7.03 | ppl  1130.80\n",
      "| epoch   1 |   144/  663 batches | lr 0.40 | ms/batch 5682.86 | loss  6.96 | ppl  1048.79\n",
      "| epoch   1 |   145/  663 batches | lr 0.40 | ms/batch 5674.39 | loss  6.88 | ppl   972.43\n",
      "| epoch   1 |   146/  663 batches | lr 0.40 | ms/batch 5698.69 | loss  6.95 | ppl  1046.68\n",
      "| epoch   1 |   147/  663 batches | lr 0.40 | ms/batch 5715.61 | loss  6.93 | ppl  1026.72\n",
      "| epoch   1 |   148/  663 batches | lr 0.40 | ms/batch 5673.31 | loss  6.93 | ppl  1020.40\n",
      "| epoch   1 |   149/  663 batches | lr 0.40 | ms/batch 5676.58 | loss  6.91 | ppl  1003.26\n",
      "| epoch   1 |   150/  663 batches | lr 0.40 | ms/batch 5697.84 | loss  6.70 | ppl   814.00\n",
      "| epoch   1 |   151/  663 batches | lr 0.40 | ms/batch 5714.42 | loss  6.94 | ppl  1031.85\n",
      "| epoch   1 |   152/  663 batches | lr 0.40 | ms/batch 5824.62 | loss  6.78 | ppl   878.91\n",
      "| epoch   1 |   153/  663 batches | lr 0.40 | ms/batch 5675.25 | loss  6.99 | ppl  1088.46\n",
      "| epoch   1 |   154/  663 batches | lr 0.40 | ms/batch 5681.15 | loss  6.86 | ppl   950.12\n",
      "| epoch   1 |   155/  663 batches | lr 0.40 | ms/batch 5694.55 | loss  7.01 | ppl  1107.39\n",
      "| epoch   1 |   156/  663 batches | lr 0.40 | ms/batch 5683.20 | loss  6.91 | ppl  1007.19\n",
      "| epoch   1 |   157/  663 batches | lr 0.40 | ms/batch 5702.72 | loss  6.93 | ppl  1023.27\n",
      "| epoch   1 |   158/  663 batches | lr 0.40 | ms/batch 5696.74 | loss  7.04 | ppl  1139.09\n",
      "| epoch   1 |   159/  663 batches | lr 0.40 | ms/batch 5831.30 | loss  6.96 | ppl  1054.37\n",
      "| epoch   1 |   160/  663 batches | lr 0.40 | ms/batch 5724.66 | loss  7.01 | ppl  1111.11\n",
      "| epoch   1 |   161/  663 batches | lr 0.40 | ms/batch 5674.26 | loss  6.78 | ppl   880.50\n",
      "| epoch   1 |   162/  663 batches | lr 0.40 | ms/batch 5727.00 | loss  6.95 | ppl  1041.55\n",
      "| epoch   1 |   163/  663 batches | lr 0.40 | ms/batch 5693.68 | loss  6.95 | ppl  1043.87\n",
      "| epoch   1 |   164/  663 batches | lr 0.40 | ms/batch 5689.33 | loss  6.90 | ppl   993.93\n",
      "| epoch   1 |   165/  663 batches | lr 0.40 | ms/batch 5656.55 | loss  6.79 | ppl   893.32\n",
      "| epoch   1 |   166/  663 batches | lr 0.40 | ms/batch 5729.62 | loss  7.07 | ppl  1173.29\n",
      "| epoch   1 |   167/  663 batches | lr 0.40 | ms/batch 5715.32 | loss  6.92 | ppl  1015.51\n",
      "| epoch   1 |   168/  663 batches | lr 0.40 | ms/batch 5725.10 | loss  6.99 | ppl  1087.51\n",
      "| epoch   1 |   169/  663 batches | lr 0.40 | ms/batch 5680.95 | loss  6.92 | ppl  1012.63\n",
      "| epoch   1 |   170/  663 batches | lr 0.40 | ms/batch 5643.89 | loss  7.04 | ppl  1147.11\n",
      "| epoch   1 |   171/  663 batches | lr 0.40 | ms/batch 5694.81 | loss  6.96 | ppl  1055.18\n",
      "| epoch   1 |   172/  663 batches | lr 0.40 | ms/batch 5687.37 | loss  6.81 | ppl   905.23\n",
      "| epoch   1 |   173/  663 batches | lr 0.40 | ms/batch 5701.17 | loss  7.00 | ppl  1096.26\n",
      "| epoch   1 |   174/  663 batches | lr 0.40 | ms/batch 5635.24 | loss  6.97 | ppl  1060.19\n",
      "| epoch   1 |   175/  663 batches | lr 0.40 | ms/batch 5597.88 | loss  6.97 | ppl  1061.84\n",
      "| epoch   1 |   176/  663 batches | lr 0.40 | ms/batch 5572.81 | loss  6.92 | ppl  1008.31\n",
      "| epoch   1 |   177/  663 batches | lr 0.40 | ms/batch 5682.02 | loss  6.96 | ppl  1052.40\n",
      "| epoch   1 |   178/  663 batches | lr 0.40 | ms/batch 5751.64 | loss  7.04 | ppl  1136.16\n",
      "| epoch   1 |   179/  663 batches | lr 0.40 | ms/batch 5698.56 | loss  6.91 | ppl  1001.08\n",
      "| epoch   1 |   180/  663 batches | lr 0.40 | ms/batch 5808.42 | loss  6.98 | ppl  1075.23\n",
      "| epoch   1 |   181/  663 batches | lr 0.40 | ms/batch 5840.95 | loss  6.96 | ppl  1052.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   182/  663 batches | lr 0.40 | ms/batch 5742.45 | loss  6.96 | ppl  1051.19\n",
      "| epoch   1 |   183/  663 batches | lr 0.40 | ms/batch 5803.31 | loss  6.91 | ppl  1004.51\n",
      "| epoch   1 |   184/  663 batches | lr 0.40 | ms/batch 5833.29 | loss  6.96 | ppl  1057.60\n",
      "| epoch   1 |   185/  663 batches | lr 0.40 | ms/batch 5712.63 | loss  6.93 | ppl  1022.15\n",
      "| epoch   1 |   186/  663 batches | lr 0.40 | ms/batch 5701.05 | loss  6.96 | ppl  1049.36\n",
      "| epoch   1 |   187/  663 batches | lr 0.40 | ms/batch 5680.78 | loss  6.88 | ppl   968.23\n",
      "| epoch   1 |   188/  663 batches | lr 0.40 | ms/batch 5689.09 | loss  6.88 | ppl   969.69\n",
      "| epoch   1 |   189/  663 batches | lr 0.40 | ms/batch 5756.58 | loss  6.86 | ppl   953.68\n",
      "| epoch   1 |   190/  663 batches | lr 0.40 | ms/batch 5882.02 | loss  6.79 | ppl   888.06\n",
      "| epoch   1 |   191/  663 batches | lr 0.40 | ms/batch 5665.69 | loss  6.88 | ppl   977.03\n",
      "| epoch   1 |   192/  663 batches | lr 0.40 | ms/batch 5733.69 | loss  6.88 | ppl   974.90\n",
      "| epoch   1 |   193/  663 batches | lr 0.40 | ms/batch 5673.91 | loss  6.89 | ppl   986.93\n",
      "| epoch   1 |   194/  663 batches | lr 0.40 | ms/batch 5705.41 | loss  6.92 | ppl  1016.81\n",
      "| epoch   1 |   195/  663 batches | lr 0.40 | ms/batch 5601.96 | loss  6.90 | ppl   989.25\n",
      "| epoch   1 |   196/  663 batches | lr 0.40 | ms/batch 5649.38 | loss  6.79 | ppl   890.41\n",
      "| epoch   1 |   197/  663 batches | lr 0.40 | ms/batch 5625.43 | loss  6.95 | ppl  1040.74\n",
      "| epoch   1 |   198/  663 batches | lr 0.40 | ms/batch 5649.15 | loss  7.02 | ppl  1115.64\n",
      "| epoch   1 |   199/  663 batches | lr 0.40 | ms/batch 5710.76 | loss  6.86 | ppl   955.85\n",
      "| epoch   1 |   200/  663 batches | lr 0.40 | ms/batch 5623.37 | loss  6.90 | ppl   997.23\n",
      "| epoch   1 |   201/  663 batches | lr 0.40 | ms/batch 5812.88 | loss  6.93 | ppl  1020.15\n",
      "| epoch   1 |   202/  663 batches | lr 0.40 | ms/batch 5700.12 | loss  6.83 | ppl   921.03\n",
      "| epoch   1 |   203/  663 batches | lr 0.40 | ms/batch 5724.81 | loss  6.85 | ppl   941.68\n",
      "| epoch   1 |   204/  663 batches | lr 0.40 | ms/batch 5715.71 | loss  6.90 | ppl   988.10\n",
      "| epoch   1 |   205/  663 batches | lr 0.40 | ms/batch 5693.23 | loss  6.80 | ppl   899.71\n",
      "| epoch   1 |   206/  663 batches | lr 0.40 | ms/batch 5641.39 | loss  6.88 | ppl   969.42\n",
      "| epoch   1 |   207/  663 batches | lr 0.40 | ms/batch 5666.01 | loss  6.91 | ppl  1005.17\n",
      "| epoch   1 |   208/  663 batches | lr 0.40 | ms/batch 5555.56 | loss  6.94 | ppl  1030.37\n",
      "| epoch   1 |   209/  663 batches | lr 0.40 | ms/batch 5610.83 | loss  6.83 | ppl   929.31\n",
      "| epoch   1 |   210/  663 batches | lr 0.40 | ms/batch 5682.51 | loss  6.84 | ppl   937.21\n",
      "| epoch   1 |   211/  663 batches | lr 0.40 | ms/batch 5619.52 | loss  6.85 | ppl   941.07\n",
      "| epoch   1 |   212/  663 batches | lr 0.40 | ms/batch 5654.29 | loss  6.88 | ppl   968.55\n",
      "| epoch   1 |   213/  663 batches | lr 0.40 | ms/batch 5634.18 | loss  6.95 | ppl  1041.63\n",
      "| epoch   1 |   214/  663 batches | lr 0.40 | ms/batch 5691.68 | loss  6.95 | ppl  1041.25\n",
      "| epoch   1 |   215/  663 batches | lr 0.40 | ms/batch 5790.35 | loss  6.77 | ppl   874.71\n",
      "| epoch   1 |   216/  663 batches | lr 0.40 | ms/batch 5654.77 | loss  6.75 | ppl   852.32\n",
      "| epoch   1 |   217/  663 batches | lr 0.40 | ms/batch 5672.46 | loss  6.75 | ppl   851.58\n",
      "| epoch   1 |   218/  663 batches | lr 0.40 | ms/batch 5645.40 | loss  6.85 | ppl   944.63\n",
      "| epoch   1 |   219/  663 batches | lr 0.40 | ms/batch 5617.08 | loss  6.78 | ppl   881.13\n",
      "| epoch   1 |   220/  663 batches | lr 0.40 | ms/batch 5628.18 | loss  6.95 | ppl  1045.09\n",
      "| epoch   1 |   221/  663 batches | lr 0.40 | ms/batch 5635.88 | loss  6.96 | ppl  1055.20\n",
      "| epoch   1 |   222/  663 batches | lr 0.40 | ms/batch 5618.06 | loss  6.88 | ppl   977.43\n",
      "| epoch   1 |   223/  663 batches | lr 0.40 | ms/batch 6397.55 | loss  6.75 | ppl   854.37\n",
      "| epoch   1 |   224/  663 batches | lr 0.40 | ms/batch 6477.09 | loss  6.82 | ppl   912.98\n",
      "| epoch   1 |   225/  663 batches | lr 0.40 | ms/batch 6516.82 | loss  6.93 | ppl  1027.61\n",
      "| epoch   1 |   226/  663 batches | lr 0.40 | ms/batch 6581.04 | loss  6.89 | ppl   984.99\n",
      "| epoch   1 |   227/  663 batches | lr 0.40 | ms/batch 6064.86 | loss  6.92 | ppl  1014.89\n",
      "| epoch   1 |   228/  663 batches | lr 0.40 | ms/batch 6087.75 | loss  6.77 | ppl   874.14\n",
      "| epoch   1 |   229/  663 batches | lr 0.40 | ms/batch 6214.01 | loss  6.94 | ppl  1031.09\n",
      "| epoch   1 |   230/  663 batches | lr 0.40 | ms/batch 6722.26 | loss  6.89 | ppl   980.75\n",
      "| epoch   1 |   231/  663 batches | lr 0.40 | ms/batch 6333.95 | loss  6.78 | ppl   880.61\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-70716cb6ef13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-275-3b5cdb40e31d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, ntokens, batch_size, lr, bptt, clip)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# TODO: Check clipping config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "epochs = 3\n",
    "lr = 0.4\n",
    "bptt = 35\n",
    "clip = 0.25\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(LSTM, train_data, ntokens, batch_size, lr, bptt, clip)\n",
    "    val_loss = evaluate(LSTM, val_data, ntokens, batch_size, bptt)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                       val_loss, np.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
